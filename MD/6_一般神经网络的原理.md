## 一般神经网络

### 基本结构

- 神经网络是深度学习最主要的实现手段，它包含**多个神经网络层**，每一层包含若干**神经元**

- 神经网络层分为**输入层**、**隐藏层**(除首层和尾层的所有层)、**输出层**，每层包含一些神经元

  经过隐藏层后，输出层中**激活值最大**的结点就是`NN`最后的选择

- **全连接**：通常相邻层的结点都是两两相连的，一个结点和上层所有结点相连的连接方法称为全连接

  一个**全连接层**，即**该层的所有结点都分别和上一层的所有结点相连**

  接下来，以全连接层(权重为二维矩阵$W$)为例讲解神经网络经典的传播过程

- 输入经过每个神经元，需要进行**加权求和、添加偏置、激活函数**三步得到输出(并作为下一层的输入)

  - 加权求和：每个输入对应一组权重，可将**输入、权重**表示为**列向量、矩阵**，分别记为$\boldsymbol x$和

    $W$；其中$W_{ij}$表示**本层第$i$个结点到下层第$j$个结点的权重**

    **到某个结点的加权求和**即为权重矩阵**某一列和输入向量的点积**

    一层结点的加权求和得到下一层输入的初步结果可记为$W^T\boldsymbol x$，是一种**线性变换**

  - 添加偏置(`bias`)：在**经过激活函数前**加上偏置，可以**控制**激活神经元的**阈值**，加速模型的拟合

    偏置会在加权求和后(即上一层结点**到下一层某一个结点$i$的交汇处**)加上，记为$b_i$，下一层所有结点的偏置也可以表示为列向量$\boldsymbol b$

    你可以想象它为本层的一个隐藏结点，该结点的输出固定为$1$，偏置即其到下一层结点的权重

  - 激活函数记为$g(x)$

- **前向传播过程**(`FP`)：向前递推计算所有层的输入，直至得到输出层的输出即为模型的预测值

  - $x_i^{(l+1)}=g_i^{(l+1)}( W_{:,i}^T\boldsymbol x^{(l)}+b_i^{(l)})$
  - $\boldsymbol x^{(l+1)}=\left[\begin{matrix}g_1^{(l+1)}( W^T_{:,1}\boldsymbol x^{(l)}+b_1)\\\vdots\\g_n^{(l+1)}( W^T_{:,n}\boldsymbol x^{(l)}+b_n)\end{matrix}\right]\xlongequal{g_i相同}g^{(l+1)}\left(W^T\boldsymbol x^{(l)}+\boldsymbol b^{(l)}\right)$

- **反向传播过程**(`BP`)：反向递推计算所有参数(指所有隐藏层及输入层的权重、偏置)的误差项

  通常通过梯度下降法计算：[梯度下降法原理](./4_优化方法总结.md)

  结论：
  $$
  \begin{align}&\boldsymbol\delta^{(l)}=\begin{cases}L'·\left(g^{(l)}\right)'\boldsymbol J_{n,1},&l为输出层,n为结点数\\ W·\boldsymbol\delta^{(l+1)}·\left(g^{(l)}\right)',&l为隐藏层\end{cases}\\&\frac{\part L}{\part W^{(l)}}=\boldsymbol x^{(l)}\left(\boldsymbol\delta^{(l+1)}\right)^T\\&\frac{\part L}{\part\boldsymbol b^{(l)}}=\boldsymbol\delta^{(l+1)}\\&P_{new}^{(l)}=P_{old}^{(l)}-\eta^{(l)}\frac{\part L}{\part P_{old}^{(l)}}\end{align}
  $$

### 全连接层

- 全连接层上面已经提到过，指本层的每一个结点都和上一层的所有结点相连接
- 全连接层适合处理**特征数量较少**的**全局特征数据**，要求各条样本之间**不含空间或时间上的相关性**
- 全连接层通常作为**任务头**，用于**生成输出**而不是中间计算，例如作为输出层
- 权重为**二维矩阵**

### 一般卷积层

- 卷积层(`Convolutional Layer`)专门用于处理网格状数据，例如图像、时序信号等

  先简单说明一下图像数据的预处理方法：将所有像素均视为特征，一个图像的所有像素的`RGB`值等排成一列作为一条样本

  因此，全连接层极其不适合处理网格数据，原因是：

  - 网格数据的特征量极多，导致使用全连接层处理时，参数量爆炸导致计算效率低下
  - 网格数据具有局部相关性，相邻数据相关程度远大于不相邻数据的相关程度，而全连接层使结点和全局数据关联起来，忽略了网格数据的这个特性
  - 平移不变性：网格数据的部分目标平移时，应该将平移前后的数据看作类似的输入，但显然全连接层做不到
  - 