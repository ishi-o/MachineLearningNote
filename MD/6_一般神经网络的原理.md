## 一般神经网络

### 基本结构

- 神经网络是深度学习最主要的实现手段，它包含**多个神经网络层**，每一层包含若干**神经元**

- 神经网络层分为**输入层**、**隐藏层**(除首层和尾层的所有层)、**输出层**，每层包含一些神经元

  经过隐藏层后，输出层中**激活值最大**的结点就是`NN`最后的选择

- **全连接**：通常相邻层的结点都是两两相连的，一个结点和上层所有结点相连的连接方法称为全连接

  一个全连接层，即**该层的所有结点都分别和上一层的所有结点相连**

- 输入经过每个神经元，需要进行**加权求和、添加偏置、激活函数**三步得到输出(并作为下一层的输入)

  - 加权求和：每个输入对应一组权重，可将**输入、权重**表示为**列向量、矩阵**，分别记为$\boldsymbol x$和

    $W$；其中$W_{ij}$表示**本层第$i$个结点到下层第$j$个结点的权重**

    **到某个结点的加权求和**即为权重矩阵**某一列和输入向量的点积**

    一层结点的加权求和得到下一层输入的初步结果可记为$W^T\boldsymbol x$，是一种**线性变换**

  - 添加偏置(`bias`)：在**经过激活函数前**加上偏置，可以**控制**激活神经元的**阈值**，加速模型的拟合

    偏置会在加权求和后(即上一层结点**到下一层某一个结点$i$的交汇处**)加上，记为$b_i$，下一层所有结点的偏置也可以表示为列向量$\boldsymbol b$

    你可以想象它为本层的一个隐藏结点，该结点的输出固定为$1$，偏置即其到下一层结点的权重

  - 激活函数记为$g(x)$

- **前向传播过程**(`FP`)：向前递推计算所有层的输入，直至得到输出层的输出即为模型的预测值

  - $x_i^{(l+1)}=g_i^{(l+1)}( W_{:,i}^T\boldsymbol x^{(l)}+b_i^{(l)})$
  - $\boldsymbol x^{(l+1)}=\left[\begin{matrix}g_1^{(l+1)}( W^T_{:,1}\boldsymbol x^{(l)}+b_1)\\\vdots\\g_n^{(l+1)}( W^T_{:,n}\boldsymbol x^{(l)}+b_n)\end{matrix}\right]\xlongequal{g_i相同}g^{(l+1)}\left(W^T\boldsymbol x^{(l)}+\boldsymbol b^{(l)}\right)$

- **反向传播过程**(`BP`)：反向递推计算所有参数(指所有隐藏层及输入层的权重、偏置)的误差项

  通常通过梯度下降法计算：[梯度下降法原理](./4_优化方法总结.md)

  结论：
  $$
  \begin{align}&\boldsymbol\delta^{(l)}=\begin{cases}L'\left(g^{(l)}\right)'\boldsymbol J_{n,1},&l为输出层,n为结点数\\ W·\boldsymbol\delta^{(l+1)}·\left(g^{(l)}\right)',&l为隐藏层\end{cases}\\&\frac{\part L}{\part W^{(l)}}=\boldsymbol x^{(l)}\left(\boldsymbol\delta^{(l+1)}\right)^T\\&\frac{\part L}{\part\boldsymbol b^{(l)}}=\boldsymbol\delta^{(l+1)}\\&P_{new}^{(l)}=P_{old}^{(l)}-\eta^{(l)}\frac{\part L}{\part P_{old}^{(l)}}\end{align}
  $$