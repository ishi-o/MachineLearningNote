### 卷积层

- 卷积层(`Convolutional Layer`)专门用于处理网格状数据或具有多维度特征信息的数据，例如时间序列(一维)、图像(二维)等

  在全连接层中图像数据的预处理方法：将所有像素均视为特征，一个图像的所有像素的`RGB`值等排成一列作为一条样本

  因此，全连接层极其不适合处理网格数据，原因是：

  - 网格数据的特征量极多，导致使用全连接层处理时，参数量爆炸导致计算效率低下
  - 网格数据具有局部相关性，相邻数据相关程度远大于不相邻数据的相关程度，而全连接层使结点和全局数据关联起来，忽略了网格数据的“形状”
  - 平移不变性：网格数据的部分目标平移时，应该将平移前后的数据看作类似的输入，但显然全连接层做不到

  卷积层解决了上述问题，并总结为三个特性：

  - 稀疏交互/稀疏连接/稀疏权重：使权重远小于输入，使模型只需要检测一些小的有意义的局部特征，减少存储需求的同时提高统计效率

  - 参数共享：在同一层卷积层中，计算不同结点的输出所用的参数(权重、偏置)是一致的

  - 平移等变：假设平移函数是使原输入的部分数据向某个方向平移的函数，那么原输入在经过平移函数后的输出和原输出将是等价的

    卷积层中的卷积运算对平移变换函数是天然等变的，对于放缩、旋转等变换，则需要额外的处理

- 卷积层一般作为**特征提取**的核心，靠近输入层的卷积层能够提取低级特征、越深的卷积层越能理解更高级别的语义特征

- 卷积层在不同深度学习框架中的命名较为统一，均为`ConvXd`的形式，表示`X`维卷积层

- 具体结构：

  - 卷积层的维度：卷积层允许接受维度不同的输入数据，卷积层的维度和输入数据的维度一致

    无论数据维度如何，卷积层始终进行局部特征提取，下面以二维卷积层`Conv2d`为例，一维或三维的卷积层可举一反三地理解

  - 输入、输出均为三维的**特征图**$X(c,h,w)$，其中$c$为通道数、$h$为特征高度、$w$为特征宽度

    **通道(`Channel`)数**是描述**特征种类数**的整数，例如`RGB`图像就是三通道的特征图

    卷积层不是通过展平，而是将数据的结构用通道数、高度、宽度表示出来，强调了数据的“形状”特性

  - 权重为四维的**滤波器集合**$W(c_{out},c_{in},h_W,w_W)$，在每个输出通道上的**三维张量**称为滤波器，又称**卷积核**，其中$c_{out/in}$为输出/输入通道数、$h_W$为核高度、$w_W$为核宽度

    某一层卷积核的数量，等同于输出通道数，和输入通道数无关

    一般来说，$h_W=w_W$且为奇数，此后统一用$K$表示

  - 偏置为向量$\boldsymbol b(c_{out})$，同一输出通道的所有元素共享一个偏置值

  - 输入与权重的运算为**卷积运算**，实际上并非数学上的卷积，而是**互相关运算**，用$\ast$表示

    在数学上的卷积运算中，核是进行了翻转的，即运算的双方在一方索引增大的同时另一方索引减小

    这种翻转的唯一目的是为卷积运算赋予**可交换性**，但对于神经网络而言它并不重要

    因为传统原因，许多实现把这种互相关运算称为卷积，所以本文也将其称为卷积，并默认其是不带翻转的卷积运算

- 卷积层的卷积运算定义为(设输入特征图为$X$，过滤器为$W$)：
  $$
  \begin{align}&(X\ast W)_{c_{out},i,j}=\sum_{k=1}^{c_{in}}\sum_{m=1}^{K}\sum_{n=1}^{K}X_{k,m+i-1,n+j-1}·W_{c_{out},k,m,n}\end{align}
  $$
  简单来说，输出特征图在通道为$c_1$上的第$i$行第$j$列元素的初步加权求和值，等同于计算：

  - 对输入特征图的**每个输入通道**$c_2$，取出以第$i$行第$j$列个元素为左上角的、和卷积核结构相同的子矩阵，求出这个子矩阵和输出通道为$c_1$、输入通道为$c_2$的卷积核的**逐元素相乘之和**
  - 将上述值求和，得到结果

  因此在这种情况下，$h_{out}=h_{in}-K+1$，$w_{out}$同理，即输入特征图的高度、宽度必须不小于卷积核的高度、宽度

- 卷积层相比于全连接层，有一些独有的超参数：

  - 卷积核的输出通道数、宽高：宽高一般相等，且为奇数

  - **步长**(`Stride`)：步长指卷积核移动的步长，上述公式中步长为$s=1$，通过增大步长，可达成压缩数据、增大感受野的效果，有时正需要损失部分信息而快速地减少数据规模，例如使$s=2$

    添加步长这个概念后，上述公式可表示为：

    $$
    \begin{align}&(X\ast W)_{c_{out},i,j}=\sum_{k=1}^{c_{in}}\sum_{m=1}^{K}\sum_{n=1}^{K}X_{k,\ m+(i-1)s,\ n+(j-1)s}·W_{c_{out},k,m,n}\\&h_{out}=\left\lfloor\frac{h_{in}-K}s\right\rfloor+1\end{align}
    $$

  - 卷积层的**填充**(`Padding`)：如上述所示，经过卷积核运算后，输出特征图的高度、宽度必然缩小，在后续的卷积层中会无法继续运算，上述这种方式也称为**有效填充**(`Valid`)，即对原输入不进行任何的填充

    这种缩小本质是一种数据丢失，边缘的信息从来无法位于子矩阵的中心，导致丢失边缘信息

    为了保留信息、扩大输出特征图的结构，会在原输入的四周填充预设值，最常见的两种是：若$s=1$则使输出和原输入结构相同、若$s>1$则使输出为原输入的一半

    有如下经典的填充方式：

    - 全零填充(`Same`)：对四周填充零
    - 反射填充(`Reflection`)：以四个边缘为对称轴，对称地填充四周的数值，顺序是左右上下

    填充后新增的圈数称为填充量，记为$p$，可以这样计算：
    $$
    \begin{align}&\because h_{out}=\left\lfloor\frac{h_{in}-K+2p}s\right\rfloor+1\\&\therefore\left\lfloor\frac{(h_{out}-1)s+K-h_{in}}2\right\rfloor\le p\lt\left\lfloor\frac{h_{out}s+K-h_{in}}2\right\rfloor\end{align}
    $$
    因此，取$\begin{align}p=\left\lfloor\frac{(h_{out}-1)s+K-h_{in}}2\right\rfloor\end{align}$即可

    当输出结构与原输入一致且步长$s=1$时，$\begin{align}p=\left\lfloor\frac{K-1}2\right\rfloor\end{align}$

- 感受野(`Receptive Field`，或译为**接受域**)：指在某一层特征图上的某一个元素，对应原始数据输入的区域大小

  输入层的每一个神经元结点的感受野$RF$都是$1$，表示该层的每一个结点都只能看到$1\cross1$的区域

  卷积程度越深，感受野将越大，直到最后某层的卷积结点的感受野等于全局感受野时，意味着一个结点掌握了完整输入的全局特征

  感受野可以迭代地计算：$\begin{align}RF_{l+1}=RF_{l}+(K_{l+1}-1)\prod_{i=1}^{l}s_i\end{align}$

  可以通过计算感受野大小，帮助我们调整上述超参数，但感受野实际上**与填充无关**

- 卷积层的前向传播：
  $$
  \begin{align}&X_{c_{in}}^{(l+1)}=Z^{(l)}_{c_{out}}=g^{(l)}\left((X^{(l)}\ast W^{(l)})_{c_{out}}+b^{(l)}_{c_{out}}\right)\end{align}
  $$

- 卷积层的反向传播梯度计算，以计算输出通道为$c_{out}$的参数梯度为例：

  为了便于理解，先从$s=1$、`Same`填充开始推导：
  $$
  \begin{align}&\frac{\part Y^{(l)}_{c_{out}}}{\part X^{(l)}_{c_{in},i,j}}=\sum_{m=1}^K\sum_{n=1}^KW_{c_{out},c_{in},m,n},&(和i,j无关,是标量,记为SW)\\&\boldsymbol\delta^{(l)}_{c_{out}}=\begin{cases}{\Large\frac{\part L}{\part\hat y}\frac{\part g^{(l)}}{\part Y^{(l)}}},&l为输出层\\\boldsymbol\delta^{(l+1)}_{c_{out}}SW{\Large\frac{\part g^{(l)}}{\part Y^{(l)}}},&l为隐藏层\end{cases}\\&\frac{\part Y^{(l)}}{\part W^{(l)}_{c_{out},c_{in},i,j}}=\sum_{m=1}^{K}\sum_{n=1}^KX_{c_{in},\ i+(m-1)s,\ j+(n-1)s}\\&\frac{\part L}{\part W^{(l)}_{c_{out},c_{in},i,j}}=\boldsymbol\delta^{(l)}_{c_{out},i,j}\frac{\part Y^{(l)}}{\part W^{(l)}_{c_{out},c_{in},i,j}}\end{align}
  $$

- 

### 池化层

