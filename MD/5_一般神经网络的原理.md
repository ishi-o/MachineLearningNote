## 一般神经网络

### 基本结构

- 神经网络是深度学习最主要的实现手段，它包含**多个神经网络层**，每一层包含若干**神经元**

- 神经网络层根据位置不同分为**输入层**、**隐藏层**(除首层和尾层的所有层)、**输出层**

- 虽然神经网络层种类众多，但输入经过每层，基本地需要进行**加权求和、添加偏置、激活函数**三步得到输出，同时作为下一层的输入

  - 加权求和：每个输入对应一组权重，分别记为$X$和$W$

    到某个结点的加权求和是一种运算，记为$W\circ X$，这个中间结果记为$Y$

  - 添加偏置(`bias`)：在**经过激活函数前**加上偏置，可以**控制**激活神经元的**阈值**，加速模型的拟合

    偏置表示为$b$

    你可以想象它为本层的一个隐藏结点，该结点的输出固定为$1$，偏置则是其到下一层结点的权重

  - 激活函数记为$g(x)$，同一层的激活函数一般相同，激活后的非线性输出为本层的最终输出，记为$Z$

- **前向传播过程**(`FP`)：向前递推计算所有层的输入，直至得到输出层的输出即为模型的预测值

  - $\begin{align}&X^{(l+1)}=Z^{(l)}=g^{(l)}(Y^{(l)})=g^{(l)}\left(W^{(l)}\circ X^{(l)}+b^{(l)}\right)\end{align}$
  - 为了方便，称参数、激活函数属于第$l$层，为在第$l$层“向前看”的参数、激活函数

- **反向传播过程**(`BP`)：反向递推计算所有参数(指所有隐藏层及输入层的权重、偏置)的误差项

  通常通过梯度下降法计算，其一般结论为：

  - $\begin{align}参数新值=参数旧值-\eta\frac{\part L}{\part{参数}}\end{align}$，其中：

    $\eta$为**学习率**，用于控制更新参数的步长，不至于使每次训练过度调整导致在最优点周围徘徊

    $L$为损失函数，$\begin{align}\frac{\part L}{\part 参数}\end{align}$为**参数梯度**，其基本原理为使参数向梯度的反方向调整，即可使模型的损失值沿最小值方向变化

  - 每层的参数梯度可通过迭代的方式计算：

    $\begin{align}&\frac{\part L}{\part 参数^{(l)}}=\frac{\part L}{\part Y^{(l)}}\frac{\part Y^{(l)}}{\part 参数^{(l)}}\\&\frac{\part L}{\part Y^{(l)}}=\frac{\part L}{\part Y^{(l+1)}}\frac{\part Y^{(l+1)}}{\part Y^{(l)}}=\frac{\part L}{\part Y^{(l+1)}}\frac{\part Y^{(l+1)}}{\part X^{(l+1)}}\end{align}$

- **计算图**：为了用正式的图形语言表示上述通过一系列运算将输入变为输出的过程，使用计算图语言，它是一个有向无环图，描述了不同层参数、输入之间的计算关系，称为计算图

### 梯度下降法及其优化

- 梯度下降法(`Gradient Descent`，`GD`)是神经网络训练中最基本的优化算法

  为了达成损失最小化，需要**使损失函数对所有参数的偏导为零**(达到极值)

  为了使该极值为极小值，需要向函数值减小的方向调整

  根据梯度的数学意义，**沿梯度的反方向调整**即可满足**导数下降**、**函数值下降**、**调整效果最大化**三个要求，故称梯度下降

- 参数更新：使原本的参数减去算出的梯度值即可(即沿梯度的反方向更新)

  为了控制每次更新的步长**不至于过大**而导致模型永远在最优点四周徘徊，一般会给每一层添加学习率$\eta$(一般为$0.001$)，即：
  $$
  \begin{align}P_{new}^{(l)}=P_{old}^{(l)}-\eta^{(l)}\frac{\part L}{\part P_{old}^{(l)}}\end{align}
  $$

- 衍生算法：

  - 随机梯度下降法(`Stochastic GD`)/增量梯度下降法(`Increment GD`)：随机选择一个样本点训练，每处理一个样本点，立刻根据梯度更新参数
    - 训练很快，能确保在局部最优解附近
    - 像一个醉鬼一样下山，走到哪算哪，可能无法走到全局最优解(损失函数有多个极小值点，可能只能走到较大的极小值点而无法走到最小值点)
  - 批量梯度下降法(`Batch GD`)：批量即每次求损失值时，计算所有样本的梯度并存储(暂时不更新)
    在处理完整个样本集后，对梯度求平均后更新
    - 每次迭代时能稳定地指向最准确的极值方向，结果是全局最优的
    - 当一个数据集的样本特别多时，一次的训练时间过长
  - 小批量梯度下降(`Mini-Batch GD`)：小批量如其名，将样本集再划分为多个小样本集，一次训练对小样本集进行上述操作
    是一种折中的方法，兼顾训练效率、模型准确度

- 
