## 常用优化方法

### 最小二乘法

- 最小二乘法针对以`MSE`作为损失函数的模型，适用于小规模线性模型
- 在误差项零均值、方差恒定、误差间不相关时，最小二乘解$\hat W=(X^TX)^{-1}X^Ty$具有最小方差

### 梯度下降法

- 梯度下降法(`Gradient Descent`，`GD`)是神经网络训练中最基本的优化算法

  为了达成损失最小化，需要**使损失函数对所有参数的偏导为零**(达到极值)

  为了使该极值为极小值，需要向函数值减小的方向调整

  根据梯度的数学意义，**沿梯度的反方向调整**即可满足**导数下降**、**函数值下降**、**调整效果最大化**三个要求，故称梯度下降

- 在说明梯度下降法之前，需要简单说明一般神经网络的结构以及前向传播过程：
  - 分为**输入层**、**隐藏层**(除首层和尾层的所有层)、**输出层**，每层包含一些神经元

    经过隐藏层后，输出层中**激活值最大**的结点就是`NN`最后的选择

  - **全连接**：通常相邻层的结点都是两两相连的，一个结点和上层所有结点相连的连接方法称为全连接

    一个全连接层，即**该层的所有结点都分别和上一层的所有结点相连**

  - 输入经过每个神经元，需要进行**加权求和、添加偏置、激活函数**三步得到输出(并作为下一层的输入)
    - 加权求和：每个输入对应一组权重，可将**输入、权重**表示为**列向量、矩阵**，分别记为$\boldsymbol x$和

      $W$；其中$W_{ij}$表示**本层第$i$个结点到下层第$j$个结点的权重**

      **到某个结点的加权求和**即为权重矩阵**某一列和输入向量的点积**

      一层结点的加权求和得到下一层输入的初步结果可记为$W^T\boldsymbol x$，是一种**线性变换**

    - 添加偏置(`bias`)：在**经过激活函数前**加上偏置，可以**控制**激活神经元的**阈值**，加速模型的拟合
    
      偏置会在加权求和后(即上一层结点**到下一层某一个结点$i$的交汇处**)加上，记为$b_i$，下一层所有结点的偏置也可以表示为列向量$\boldsymbol b$
    
      你可以想象它为本层的一个隐藏结点，该结点的输出固定为$1$，偏置即其到下一层结点的权重
    
    - 激活函数记为$g(x)$
    
  - **前向传播过程**(`FP`)：递推计算所有层的输入

    - $x_i^{(l+1)}=g_i^{(l+1)}( W_{:,i}^T\boldsymbol x^{(l)}+b_i^{(l)})$
    - $\boldsymbol x^{(l+1)}=\left[\begin{matrix}g_1^{(l+1)}( W^T_{:,1}\boldsymbol x^{(l)}+b_1)\\\vdots\\g_n^{(l+1)}( W^T_{:,n}\boldsymbol x^{(l)}+b_n)\end{matrix}\right]\xlongequal{g_i相同}g^{(l+1)}\left(W^T\boldsymbol x^{(l)}+\boldsymbol b^{(l)}\right)$

- 梯度下降法原理推导(核心是链式求导)：

  为了描述方便，假设第$l+1$层为输出层，即$x_i^{(l+1)}$就是模型最终的预测值

  先考察$L$对第$l$层参数的梯度，这一层的权重/偏置是特殊的(以权重为例)：

  因为任意一个$W_{ij}^{(l)}$只和$x_j^{(l+1)}$相关，即只和$L_j$相关

  于是$\begin{align}&\frac{\part L}{\part W_{ij}^{(l)}}=\frac{\part L_j}{\part W_{ij}^{(l)}}=\frac{\part L_j}{\part x_j^{(l+1)}}\frac{\part x_j^{(l+1)}}{\part W_{ij}^{(l)}}\end{align}$，左边即损失函数的导数$L'$

  由于$\begin{align}&x_j^{(l+1)}=g_j^{(l+1)}\left(W_{:,j}^T\boldsymbol x^{(l)}+b_j^{(l)}\right)\end{align}$，我们令$\begin{align}u_j^{(l)}=W_{:,j}^T\boldsymbol x^{(l)}+b_j^{(l)}\end{align}$，即$\begin{align}\sum_{l\in l}W_{lj}x_l^{(l)}+b_j^{(l)}=W_{ij}x_i^{(l)}+C\end{align}$，$C$为和$W_{ij}^{(l)}$无关的常数项

  则$\begin{align}原式=L'\frac{\part g_j^{(l+1)}}{\part u_j^{(l)}}\frac{\part u_j^{(l)}}{\part W_{ij}^{(l)}}=L'\left(g_j^{(l+1)}\right)'x_i^{(l)}\end{align}$

  对于第$l$层的偏置，结果是类似的，只不过由于$\begin{align}\frac{\part x_j^{(l+1)}}{\part b_j^{(l)}}=1\end{align}$，梯度为$L'\left(g_j^{(l+1)}\right)'$

  再考察$L$对前$l-1$层参数的梯度，由于第$l+1$层为全连接层，第$l$层的输出的任意改动会影响所有$L_j$，自然而然地，前$l-1$层任意参数的梯度会涉及到所有$L_j$

  但求解并没有变得多复杂，因为总损失$\begin{align}L=\sum_{i\in l+1}L_i\end{align}$只是单纯的求和，所以(让我们以$W_{ij}^{(l-1)}$为例)$\begin{align}&\frac{\part L}{\part W_{ij}^{(l-1)}}=\sum_{l\in l+1}\frac{\part L_l}{\part W_{ij}^{(l-1)}}\end{align}$也是一个单纯的求和，其中每一项：
  $\begin{align}&\frac{\part L_l}{\part W_{ij}^{(l-1)}}=\frac{\part L_l}{\part x_{j}^{(l)}}·\frac{\part x_j^{(l)}}{\part W_{ij}^{(l-1)}}=\frac{\part L_l}{\part x_{l}^{(l+1)}}\frac{\part x_l^{(l+1)}}{\part u_l^{(l)}}\frac{\part u_l^{(l)}}{\part x_j^{(l)}}·\frac{\part x_j^{(l)}}{\part u_j^{(l-1)}}\frac{\part u_j^{(l-1)}}{\part W_{ij}^{(l-1)}}\\&=L'\left(g_{l}^{(l+1)}\right)'W_{jl}^{(l)}·\left(g_j^{(l)}\right)'x_i^{(l-1)}\end{align}$

  于是$\begin{align}&原式=L'\left(\sum_{l\in l+1}\left(g_l^{(l+1)}\right)'W_{jl}^{(l)}\right)\left(g_j^{(l)}\right)'x_i^{(l-1)}\end{align}$

  过程是很繁琐的，但熟悉链式法则的话，展开是很顺畅的

- 公式简化：

  实际上，最复杂的那一项$\begin{align}\frac{\part L}{\part x^{(l)}}\end{align}$可以通过**迭代**的方式表现出来，将其记为$\delta^{(l)}$，称为**局部梯度**，或称**反向传播误差**
  $$
  \begin{align}\delta^{(l)}_i=\begin{cases}L'\left(g_i^{(l)}\right)',&l为输出层\\ W_{i,:}\boldsymbol\delta^{(l+1)}\left(g_i^{(l)}\right)',&l为隐藏层\end{cases}\end{align}
  $$
  简单来说就是，$\boldsymbol\delta^{(l)}$是一个列向量，输出层的$\boldsymbol\delta$每一项都是$L'\left(g_i^{(l)}\right)'$，往前所有层的$\boldsymbol\delta$的第$i$项是该层权重矩阵的**第$i$行**和后一层$\boldsymbol\delta$的点积再乘上该层第$i$个激活函数的导数

  若同一层的所有激活函数相同，则可以表示为：
  $$
  \begin{align}\delta^{(l)}=\begin{cases}L'\left(g^{(l)}\right)',&l为输出层\\ W·\boldsymbol\delta^{(l+1)}·\left(g^{(l)}\right)',&l为隐藏层\end{cases}\end{align}
  $$
  有了这个记号，前面求的梯度都可以简化，例如：
  $\begin{align}&\frac{\part L}{\part W_{ij}^{(l)}}=\delta_j^{(l+1)}x_i^{(l)}、\frac{\part L}{\part b_j^{(l)}}=\delta_j^{(l+1)}\end{align}$

- 参数更新：使原本的参数减去算出的梯度值即可(即沿梯度的反方向更新)

  为了控制每次更新的步长**不至于过大**而导致模型永远在最优点四周徘徊，一般会给每一层添加学习率$\eta$(一般为$0.001$)，即：
  $$
  \begin{align}P_{new}^{(l)}=P_{old}^{(l)}-\eta^{(l)}\frac{\part L}{\part P_{old}^{(l)}}\end{align}
  $$

- 衍生算法：

  - 随机梯度下降法(`Stochastic GD`)/增量梯度下降法(`Increment GD`)：随机选择一个样本点训练，每处理一个样本点，立刻根据梯度更新参数
    - 训练很快，能确保在局部最优解附近
    - 像一个醉鬼一样下山，走到哪算哪，可能无法走到全局最优解(损失函数有多个极小值点，可能只能走到较大的极小值点而无法走到最小值点)
  - 批量梯度下降法(`Batch GD`)：批量即每次求损失值时，计算所有样本的梯度并存储(暂时不更新)
    在处理完整个样本集后，对梯度求平均后更新
    - 每次迭代时能稳定地指向最准确的极值方向，结果是全局最优的
    - 当一个数据集的样本特别多时，一次的训练时间过长
  - 小批量梯度下降(`Mini-Batch GD`)：小批量如其名，将样本集再划分为多个小样本集，一次训练对小样本集进行上述操作
    是一种折中的方法，兼顾训练效率、模型准确度


### 动量优化法

### 自适应学习率法

### 二阶优化法

### 模型架构的优化(超参数优化)